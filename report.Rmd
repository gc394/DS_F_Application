---
title: "F Application Report"
author: "Greg Cooke"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999, dplyr.summarise.inform = F)
```


```{r libraries, include=FALSE}

library(openxlsx)
library(tibble)
library(magrittr)
library(tidyr)
library(purrr)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(glue)
library(scales)
library(ggrepel)
library(corrplot)
library(forcats)
library(tidymodels)

```

In this report I will look to highlight the firms that should be focused on by our supervisors using key metrics that I will explain throughout the report. I will keep all my code in the report for audit and reusability purposes as conclusions may change as new data arrives. I will focus this piece on the three main characteristics stated: Firm Size, Changing Business Profile and Outliers.

## Data Loading and Wrangling

I have used a combination of *openxlsx* and *tidyverse* packages to render the xlsx into a R-readable output. Below I have produced a summary of all the variables.

```{r ETL, echo = FALSE}

# create workbook object
wb = openxlsx::loadWorkbook(file = 'DataScientist_009749_Dataset.xlsx')

# both sheets are formatted the same way so we want to
# build a function to stop code repetition!
wrangle_dataframe = function(df){
  
  # take the date vector, remove the first item which is NA (missing firm) to re-add in
  dates_vec = unname(df[1, 2:ncol(df)]) %>% 
    base::as.character() %>%
    gsub(pattern = '[A-z]', replacement = '', x = .)
  
  output_df = df %>%
    # pivot the metrics into a column
    tidyr::pivot_longer(cols = -X1) %>%
    # removes the first row
    dplyr::filter(!is.na(X1)) %>%
    # add in year from the general dates vector created earlier
    # we need to repeat this for each firm 
    dplyr::mutate(year = rep(dates_vec, times = length(unique(.$X1))),
                  value = as.numeric(value)) %>%
    # rename and make easier for me to read
    dplyr::select(firm = X1, metric = name, year, value)
  
  # due to volatility we want to create descriptive metrics to find biggest, most volatile etc. firms
  added_metrics_output_df = output_df %>%
    # we are going to describe the whole time series so just need firm and metric
    dplyr::group_by(firm, metric) %>%
    dplyr::summarise(
      # this are the variables I will use (for now)
      mean_value = mean(value, na.rm = T),
      median_value = median(value, na.rm = T),
      sd_value = sd(value, na.rm = T),
      # mad_value = mad(value, na.rm = T)
      )
  
  # add them back to the original output and then pivot longer
  # creating 'value type' variable - only value element varies temporarily
  output_df = output_df %>%
    dplyr::left_join(
      added_metrics_output_df,
      by = c('firm', 'metric')) %>%
    tidyr::pivot_longer(
      cols = contains('value'), 
      names_to = 'value_type'
    ) %>%
    dplyr::mutate(
      # renaming value to time_series_value for ease
      value_type = case_match(value_type, 
                              'value' ~ 'time_series_value', 
                              .default = value_type))
    
  return(output_df)
  
}

# read, wrangle and combine datasets
df = purrr::map(
  .x = openxlsx::sheets(wb),
  .f = function(x){
    
    # read data in
    openxlsx::readWorkbook(
      xlsxFile = wb, 
      sheet = x) %>%
      # wrangle using function defined above
      wrangle_dataframe()
    
  }) %>%
  # purrr recommends this approach over map_dfr due to edge cases from dplyr::bind_rows()
  purrr::list_rbind() %>%
  # just to make it easier to read for me - again.
  dplyr::arrange(firm, metric, year, value_type) %>%
  # each metric to it's own column
  tidyr::pivot_wider(
    names_from = metric, 
    values_from = value) %>%
  # clean names does most of the heavy lifting
  janitor::clean_names() %>%
  # final touch ups
  dplyr::rename(
    eof_for_scr_m = eo_f_for_scr_m,
    equity_m = excess_of_assets_over_liabilities_m_equity,
    gross_bel_m = gross_bel_inc_t_ps_as_whole_pre_tmtp_m,
    net_bel_m = net_bel_inc_t_ps_as_a_whole_pre_tmtp_m,
  )

# pretty table output
knitr::kable(summary(df))  %>%
  kable_styling("striped", full_width = F) %>% 
  scroll_box(width = "100%")
```

From this summary we can see each of the columns are in the correct data type and there is significant variance across the numeric variables. The dataset looks like this now:

```{r, echo = FALSE}

# pretty table output
knitr::kable(head(df, 5))  %>%
  kable_styling("striped", full_width = F) %>% 
  scroll_box(width = "100%")

```


I have set up the datatable so that only *value_type* *time_series_value* varies over time, all the other variables in this column stay consistent. This makes it straightforward to wrangle to find the top/bottom n firms or most/least varying firms by variable as you will see in my visualisation code.


## Caveats

```{r Missing Firms, eval=FALSE}

pull_unique_firms = function(wb, sheet_n){
  
  openxlsx::readWorkbook(
      xlsxFile = wb, 
      sheet = openxlsx::sheets(wb)[sheet_n])['X1'] %>%
    dplyr::pull()
    
}

firms_sheet1_vec = pull_unique_firms(wb, 1)
firms_sheet2_vec = pull_unique_firms(wb, 2)
  
length(setdiff(firms_sheet2_vec, firms_sheet1_vec))
```


There are some caveats which will be worth considering with my data-centric approach to resource allocation, these are:

* Data Quality, I can perform some degree of data cleaning and provide a high level of confidence in data quality however this is not perfect.
* Missing data, we see that 131 more firms (Firm 326 to Firm 456 inclusive) report in the *Underwriting* sheet than in the *General* sheet of the XLSX.

## Correlation Plot

```{r Corr Matrix, echo = FALSE}

corr_mat = df %>%
  # take the mean value
  dplyr::filter(value_type == 'median_value') %>%
  dplyr::select(-year, -value_type, -firm) %>%
  # remove duplications from value_types
  dplyr::distinct() %>%
  stats::cor(., method = "pearson", use = "complete.obs")

corrplot(corr_mat, type = "upper", 
         order = "hclust", 
         tl.col = "black", 
         tl.srt = 45)

```


## Firm Size

To consider firm size I will look at:

* *Total Assets*, as assets represent the resources these firms own to generate revenue from.
* *Net Written Premium* this is a better indicator than Gross in my opinion as it considers income post reinsurance costs giving a better picture of profit.
* *Equity*, this represents the amount of money returned to shareholders if the insurer liquidates. This is important as it shows a picture of financial health and stability.

As we want to consider both magnitude and consistency of the variables, I will use the median score to select the highest rated firms and will plot them against the rest.

```{r Total Assets Plot, echo=FALSE}

plot_metric_by_top_n_type = function(df, metric, type, n){
  
  # find biggest median asset firms
grouped_df = df %>%
  dplyr::filter(value_type == type) %>%
  dplyr::select(all_of(c('firm', metric))) %>%
  dplyr::group_by(firm) %>%
  dplyr::summarise(m = max(.data[[metric]]))

# find the top 3 firms
topn_firms = grouped_df %>%
  dplyr::slice_max(m, n = n) %>%
  dplyr::pull('firm')

# find the top 20pct of firms
top20pct_firms = grouped_df %>%
  dplyr::slice_max(m, prop = -.80) %>%
  dplyr::pull('firm')

# produce color vectors for the top three firms
# this gives the colour values of the vector
cols = c(ggthemes::wsj_pal()(length(topn_firms)), 
         rep('black', length(unique(df$firm)) - length(topn_firms)))

# by then naming the vector we can set the color
names(cols) = c(topn_firms, unique(df$firm)[!unique(df$firm) %in% topn_firms])

# graph
ggplot2::ggplot(
  data = df %>%
    # helps to read and debug having a smaller dataset
    dplyr::select(all_of(c('firm', 'year', 'value_type', metric))) %>%
    # we want to use the time series value as this one changes over time and we only want top 20%
    # of firms to compare this to
    dplyr::filter(value_type == 'time_series_value', firm %in% top20pct_firms) %>%
    dplyr::mutate(
      alpha = case_when(
        firm %in% topn_firms ~ 1,
        .default = 0.9
        ),
      # width = case_when(
      #   firm %in% topn_firms ~ 0.00021,
      #   .default = 0.0002
      # ),
      text = case_when(
        firm %in% topn_firms & year == '2020' ~ firm,
        .default = ''
        )
    ),
  mapping = aes(
    x = year, 
    y = .data[[metric]], 
    # linewidth = width,
    group = firm,
    alpha = alpha,
    color = firm,
    label = text)
)  +
  # ggthemes::theme_fivethirtyeight() +
  geom_line() + 
  # creates labels that don't bump into each other
  geom_label_repel(aes(label = text),
                   nudge_x = 1,
                   na.rm = TRUE) +
  # sets colors on for topn firms
  scale_color_manual(
    values = cols
  ) + 
  scale_y_continuous(#scales::label_number_auto()
    labels = scales::label_number_auto()) + 
  theme(legend.position = "none") +
  labs(
    title = glue::glue('{metric} over time'),
    subtitle = glue::glue('{n} largest firms by {gsub("_", " ", type)} highlighted compared with the top 20%'),
    x = 'Year End',
    y = case_when(
      grepl(pattern = '_m$', x = metric) ~ 'GBP (M)',
      grepl(pattern = 'ratio$', x = metric) ~ 'Ratio'
    ))

}

plot_metric_by_top_n_type(df = df, metric = 'total_assets_m', type = 'median_value', n = 5)
```

From this graph we can see there are five firms that are distinctly larger than the rest and I have only considered the top 20% of firms by median value. It is worth highlighting, and I will explore later, the volatility of some of the firms accounts as we can see firm 311 and 210 have large drops in 2020 (YE). 

```{r NWP, echo = FALSE}

plot_metric_by_top_n_type(df = df, metric = 'nwp_m', type = 'median_value', n = 4)

```

Only Firms 210 and 4 are considerably larger than the rest of the top 20% of firms (for this variable). 

```{r Equity, echo = FALSE}

plot_metric_by_top_n_type(df = df, metric = 'equity_m', type = 'median_value', n = 4)

```
Firm 4 has appears twice now, firstly for having a significantly larger Net Written Premium and now for Equity. This suggests to us that it is a largely profitable insurance firm with a large equity holding meaning it is in a very healthy financial position. I would suggest this firm is definitely an important player in the market and one to monitor.

```{r Size Table, echo = FALSE}

# we want to find the top sd firms for all size metrics and make a table for the top 15/10%
size_table_df = df %>%
  # take the mean value
  dplyr::filter(value_type == 'median_value', firm %in% glue('Firm {1:325}')) %>%
  dplyr::select(-year, -value_type) %>%
  # remove duplications from value_types
  dplyr::distinct() %>%
  # find standard deviation for each firm 
  dplyr::mutate(
    across(
      .cols = where(is.numeric),
      # find the absolute sd of each of the variable columns
      .fns = ~ abs(sqrt((.x - mean(.x, na.rm = T))^2 / length(.x))), 
      .names = '{.col}_abs_sd'),
    # normalise these columns to stop weighting bias
    across(
      .cols = ends_with('_abs_sd'),
      .fns = ~ abs(sqrt((.x - mean(.x, na.rm = T))^2 / length(.x))), 
      .names = '{.col}_normalised')
    ) %>% 
  rowwise() %>%
  # have used select variables to determine size 
  dplyr::mutate(size_score = mean(c(nwp_m_abs_sd_normalised, net_bel_m_abs_sd_normalised, pure_gross_claims_ratio_abs_sd_normalised), na.rm= T)) %>%
  dplyr::arrange(desc(size_score)) %>%
  utils::head(10) %>%
  dplyr::select(firm, size_score, nwp_m_abs_sd_normalised, net_bel_m_abs_sd_normalised, pure_gross_claims_ratio_abs_sd_normalised)

# pretty table output
knitr::kable(size_table_df)  %>%
  kable_styling("striped", full_width = F) %>% 
  scroll_box(width = "100%")

```

I have created a table here that provides a potential list of the top 10 firms **using only firms that reported on the general template also**. I have calculated this by taking the *mean normalised absolute standard deviation of the median value* for each firm on specific metrics. To explain this, I first take the median, because mean is more affected by outliers, value of each metric for each firm and then find the absolute standard deviation against the metric. This allows me to find the firms that are, on average, far above the population for each metric. As the distributions vary for each of the metrics I then normalise each of this scores so that when I take the mean there is no weighting bias. I chose the variable *Net Weighted Premium*, *Net BEL (inc. TPs as a whole, pre-TMTP)* and *Pure Gross Claims Ratio* because as you can see from my correlation matrix above, these three variables explain a lot of the variance of the entire dataset and I felt they were therefore wide ranging and mutually exclusive enough to consider all aspects of the business. It is worth noting as you can see from my outputs there is an issue of possible outliers that I will consider and explore later.

## Changing Business Profiles

Volatility of a firm's accounts are of considerable concern to Supervisors and so it's important to investigate. 

I will start with considering variation the metric level. I am going to use *relative standard deviation* as this will allow me to compare SDs across each of the variables effectively.

```{r RSD of Variables, echo = FALSE}


rasd_metrics_df = df %>%
  dplyr::filter(value_type == 'time_series_value') %>%
  tidyr::pivot_longer(
    cols = -c('firm', 'year', 'value_type')
    ) %>%
  dplyr::group_by(year, name) %>%
  dplyr::summarise(
    mean = mean(value, na.rm = T),
    number = n(),
    rasd = abs(sd(value, na.rm = T) * 100 / mean(value, na.rm = T))
  )

top3 = rasd_metrics_df %>%
  dplyr::group_by(year, name) %>%
  dplyr::summarise(rasd = max(rasd)) %>%
  dplyr::slice_max(rasd, n = 3) %>%
  dplyr::mutate(text = name) %>%
  dplyr::select(-rasd)

# produce color vectors for the top three firms
# this gives the colour values of the vector
# cols = c(ggthemes::wsj_pal()(length(unique(top3$name))), 
#          rep('black', (length(unique(rsd_metrics_df$name)) - length(unique(top3$name)))))
# 
# # by then naming the vector we can set the color
# names(cols) = c(unique(top3$name), unique(rsd_metrics_df$name)[!unique(rsd_metrics_df$name) %in% unique(top3$name)])

ggplot(
  rasd_metrics_df %>%
    dplyr::left_join(
      top3, by = c('year', 'name')
    ), 
  aes(x = fct_reorder2(name, year, rasd , .desc = F),
      y = rasd,
      fill = name,
      text = text)) + 
  geom_col() +
  facet_wrap(~year, 
             ncol = 1, 
             scales = 'free_y')  + 
  scale_y_continuous(
    labels = scales::label_number(scale_cut = scales::cut_short_scale(),
                                  accuracy = 1), 
    breaks = scales::extended_breaks(n = 3)) + 
  geom_label_repel(
    aes(label = text),size = 2,
                   box.padding = unit(0.25, "lines"),
    nudge_x = 5,
    nudge_y = 200,
    na.rm = TRUE) + 
  theme(legend.position = "none", axis.text.x = element_blank()) +
  labs(
    title = glue::glue('RASD of Metrics'),
    subtitle = glue::glue('Split by year'),
    x = 'Metrics',
    y = 'Relative Absolute Standard Deviation'
    )

```

This chart shows us that RSD is varies across metrics over time and that there are multiple metrics with high variance however there are six main metrics that have a high relative standard deviation all of which are ratios these are: *gross_expense_ratio*, *net_expense_ratio*, *gross_combined_ratio*, *net_combined_ratio*, *pure_gross_claims_ratio*, and *pure_net_claims_ratio*. Although there are 6 they are 3 highly correlated net and gross pairs. There are also strong relationships by definition. *net_combined_ratio* is $\frac{Sum of net claims and expenses incurred}{Net Earned Premium}$ while *net_expense_ratio* is just the expenses part of the equation suggesting this could be an expenses variance. We also see a massive spike in 2016 for RSD in *gross_expense_ratio* which I would suspect is misreporting especially given no spike in *net_expense_ratio* and I will explore later. I will look into the firms that have produced these high variations in the net variables as these will likely explain the variations in the gross variables also.

```{r Net Expense Ratio pre-filter, echo =FALSE}

plot_metric_by_top_n_type(df = df, metric = 'net_expense_ratio', type = 'sd_value', n = 5)

```
We can see from this graph that *net_expense_ratio* spikes has potentially come from these two enormous values that have been reported. I will add a filter to this chart and the other ratio charts to have median values between 0 and 0.87 (this is the range set by 3Q + 1.5 * IQR). I will be consistent in flagging and filtering outliers in this way.

```{r Net Expense Ratio post-filter, echo =FALSE}

remove_firm_outliers_for_ratios = function(df, var, min_threshold = 0, max_threshold = 1){
  
 vec = df %>% 
  dplyr::filter(value_type == 'median_value') %>% 
  dplyr::pull(var) 

  upper_limit = quantile(vec, 0.75, na.rm = T) + (1.5 * IQR(vec, na.rm = T))
  
  firms = df %>%
    dplyr::filter(value_type == 'time_series_value' &
                    # take the minimum of upper limit or max_threshold
                  .data[[var]] < min(upper_limit, max_threshold) &
                  .data[[var]] > min_threshold) %>%
    group_by(firm) %>%
    # we only want to consider complete time series
    dplyr::filter(n() == 5) %>%
    dplyr::pull(firm) 
    
  return(firms)  

}

firms = remove_firm_outliers_for_ratios(df, 'net_expense_ratio')

plot_metric_by_top_n_type(df = df %>%
                            dplyr::filter(firm %in% firms),
                          metric = 'net_expense_ratio', 
                          type = 'sd_value', 
                          n = 5)

```
After filtering out misreporting firms, we can see the five firms whose data has changed most substantially over the period 2016-20. This is important for supervisors to be aware of because its ability to damage investors confidence in the firm as well as conduct any reasonable long-term financial planning.


```{r Net Combined Ratio pre-filter, echo =FALSE}

plot_metric_by_top_n_type(df = df, metric = 'net_combined_ratio', type = 'sd_value', n = 5)

```
As Firm 418 has misreported both *net_combined_ratio* and *net_expense_ratio*, we can assume the issue is in a misreporting of their expenses which has had a knock-on impact on multiple of their larger KPIs.

```{r Net Combined Ratio post-filter, echo =FALSE}

firms = remove_firm_outliers_for_ratios(df, 'net_combined_ratio', max_threshold = 2)

plot_metric_by_top_n_type(df = df %>%
                            dplyr::filter(firm %in% firms),
                          metric = 'net_combined_ratio', 
                          type = 'sd_value', 
                          n = 5)

```
Again, once these are removed we can see that multiple firms with volatile ratios. Firms 287, 291 and 417 having both a volatile and high *net combined ratio* would be cause for concern for supervisors.

```{r Net Pure Claims Ratio, echo =FALSE}

firms = remove_firm_outliers_for_ratios(df, 'pure_net_claims_ratio', max_threshold = 1.5)

plot_metric_by_top_n_type(df = df %>%
                            dplyr::filter(firm %in% firms),
                          metric = 'pure_net_claims_ratio', 
                          type = 'sd_value', 
                          n = 5)

```

This is calculated as $\frac{Net Claims}{Net Earned Premium}$ and so a firm like 38 who has managed to reduce this from a poor position is a positive and something that would stop concern from supervisors.

## Outliers and Misreporting

I've shown a couple of techniques I have used to filter out outliers and highlight elements in the time series. I have chosen to use the interquartile range method due to the presence of extreme outliers in the data. Standard Deviation is influenced much more strongly by extreme outliers. I have created a flagging system, if the data point is outside of $(Q1 - 1.5 \times IQR, Q3 + 1.5 \times IQR,)$ for the entire metric population then it is a *'IQR Outlier'*, if it is outside of the 2nd or 8th quantile then it is labelled a *'Decile Outlier'* else it has *'No Flag'*. I will provide an example below for firm 12.

```{r Outliers, echo = FALSE, warning=FALSE}

thresholds_df = list(
  # IQR Upper
  df %>%
    dplyr::filter(value_type == 'time_series_value') %>%
    dplyr::group_by(year, .drop = F) %>%
    dplyr::summarise(
      across(
        .cols = where(is.numeric),
        # find the % change
        .fns = ~ quantile(.x, 0.25, na.rm = T) - (1.5 * IQR(.x, na.rm = T))
        )
      ) %>%
    tidyr::pivot_longer(
      cols = -year,
      names_to = 'metric',
      values_to = 'iqr_lt'
    ),
  # IQR Lower
  df %>%
    dplyr::filter(value_type == 'time_series_value') %>%
    dplyr::group_by(year, .drop = F) %>%
    dplyr::summarise(
      across(
        .cols = where(is.numeric),
        # find the % change
        .fns = ~ quantile(.x, 0.75, na.rm = T) + (1.5 * IQR(.x, na.rm = T))
      )
    ) %>%
    tidyr::pivot_longer(
      cols = -year,
      names_to = 'metric',
      values_to = 'iqr_ut'
    ),
  # Decile Upper
  df %>%
    dplyr::filter(value_type == 'time_series_value') %>%
    dplyr::group_by(year, .drop = F) %>%
    dplyr::summarise(
      across(
        .cols = where(is.numeric),
        # find the % change
        .fns = ~ quantile(.x, 0.8, na.rm = T)
      )
    ) %>%
    tidyr::pivot_longer(
      cols = -year,
      names_to = 'metric',
      values_to = 'd_ut'
    ),
  # Decile Lower
  df %>%
    dplyr::filter(value_type == 'time_series_value') %>%
    dplyr::group_by(year, .drop = F) %>%
    dplyr::summarise(
      across(
        .cols = where(is.numeric),
        # find the % change
        .fns = ~ quantile(.x, 0.2, na.rm = T)
      )
    ) %>%
    tidyr::pivot_longer(
      cols = -year,
      names_to = 'metric',
      values_to = 'd_lt'
    )
  ) %>% 
  purrr::reduce(
    left_join, 
    by = c('year', 'metric'))


outliers_df = df %>%
  dplyr::filter(value_type == 'time_series_value')%>%
  tidyr::pivot_longer(
    cols = -c('year', 'firm', 'value_type'),
    names_to = 'metric',
    values_to = 'value'
  ) %>%
  dplyr::left_join(
    thresholds_df,
    by = c('year', 'metric')
  ) %>%
  dplyr::mutate(
    flag = case_when(
      value > iqr_ut | value < iqr_lt ~ 'IQR Outlier',
      value > d_ut | value < d_lt ~ 'Decile Outlier',
      .default = 'No Flag'
    )
  ) %>%
  dplyr::select(firm, year, metric, flag) %>%
  tidyr::pivot_wider(
    names_from = 'metric',
    values_from = 'flag'
  )
```

```{r Outliers by Metric, echo = FALSE}

flag_vec = c('IQR Outlier', 'Decile Outlier', 'No Flag')
cols = ggthemes::wsj_pal()(3)
names(cols) = flag_vec

ggplot(data = outliers_df %>%
         tidyr::pivot_longer(
           cols = -c('firm', 'year'),
           names_to = 'metric',
           values_to = 'flag') %>%
         dplyr::mutate(
           flag = factor(flag, levels = flag_vec),
           metric = gsub(pattern = '_outlier_flag', replacement = '', metric)
           ) %>%
         dplyr::group_by(metric, flag) %>%
         dplyr::summarise(count = n()),
       mapping = aes(
         x = fct_reorder(metric, count, .desc = T),
         y = count,
         fill = flag)) +
  geom_col(position = 'stack') + 
  scale_fill_manual(
    values = cols
  ) +
  scale_x_discrete(guide = guide_axis(angle = 60)) +
  theme(axis.title.x = element_text()) + 
  labs(
    title = 'Decomposition of Metric by Flag', 
    subtitle = 'By count', 
    y = '', 
    x = '') + 
  guides(fill = guide_legend(title="Flag"))

```

We can see there is fair homogeneity in submissions and there is no systemic reporting issues towards any particular metric.

This approach will allow users to do is to flagged submissions quickly, if we take Firm 139's *scr_m* and Firm 222's *nwp_m* submissions we can see that they have a range of flags throughout the series.

```{r Outlier Firms table example, echo = FALSE}

df %>%
  dplyr::filter(firm %in% c('Firm 139', 'Firm 222'), value_type == 'time_series_value') %>%
  dplyr::select(firm, year, scr_m, nwp_m) %>%
  knitr::kable() %>%
  kable_styling("striped", full_width = F)

```

If we graph that we can see this:

```{r Outlier Firm 139 chart example, echo = FALSE}

plot_outlier_firm_vs_average = function(df, example_firm, metric){
  
  firm_versus_average_df = rbind(
  df %>%
    dplyr::filter(firm == example_firm, value_type == 'time_series_value') %>%
    dplyr::select(all_of(c('firm', 'year', metric))) %>% 
    stats::setNames(c("firm","year", 'value'))  %>%
    dplyr::mutate(text = outliers_df[outliers_df$firm == example_firm,][[metric]]),
  df %>%
      dplyr::filter(firm != example_firm, value_type == 'time_series_value') %>%
      dplyr::group_by(year) %>%
      dplyr::summarise(value = median(.data[[metric]], na.rm = T),
                       text = '',
                       firm = 'Agg')
  )
  
  cols = c(ggthemes::wsj_pal()(1),
           ggthemes::wsj_pal()(2)[2]) %>%
    set_names(example_firm, 'Agg')
  
  # graph
  ggplot2::ggplot(
    data = firm_versus_average_df,
    mapping = aes(
      x = year, 
      y = value, 
      group = firm,
      color = firm,
      label = text)
  )  +
    geom_line() + 
    # creates labels that don't bump into each other
    geom_text(nudge_y = 0.6, nudge_x = -0.08, color = 'black') + 
    # sets colors on for topn firms
    scale_color_manual(
      values = cols, 
      name = 'Submission'
    ) +
    scale_y_continuous(#scales::label_number_auto()
      labels = scales::label_number_auto()) + 
    labs(
      title = glue::glue('{example_firm} submission {metric} time series'),
      subtitle = glue::glue('With respective outlier flags'),
      x = 'Year End',
      y = 'GBP (M)'
    )
  
}

plot_outlier_firm_vs_average(df = df, example_firm = 'Firm 139', metric = 'scr_m')

```

```{r Outlier Firm 222 chart, echo = FALSE}

plot_outlier_firm_vs_average(df = df, example_firm = 'Firm 222', metric = 'nwp_m')


```

So this method allows us to look at each firm's submission against the rest of the population and detect whether we consider it an outlier or not. In this case we can safely assume the IQR Outlier will require a resubmission.

For the machine learning calculations, I am going to remove all IQR outliers to stop poor data quality impoacting the outputs.

```{r filter out Outliers, echo = TRUE}

filtered_df = dplyr::anti_join(
  x = df %>%
    filter(value_type == 'time_series_value') %>%
    pivot_longer(cols = -c('firm', 'year', 'value_type')),
  y = outliers_df %>%
    pivot_longer(cols = -c('firm', 'year')) %>%
    filter(value == 'IQR Outlier'),
  # remove any that match firm , year and name
  by = c('firm', 'year', 'name')
  ) %>%
  # we are going to describe the whole time series so just need firm and metric
  dplyr::group_by(firm, name) %>%
  dplyr::summarise(
    # this are the variables I will use (for now)
    median_value = median(value, na.rm = T),
    # mad_value = mad(value, na.rm = T)
  ) %>% 
  dplyr::filter(
    !is.na(median_value)
  ) %>%
  tidyr::pivot_wider(
    names_from = name, 
    values_from = median_value) %>%
  # remove any rows with NAs - this will reduce the dataset but will lead to cleaner results
  stats::na.omit()

 kable(head(filtered_df)) %>%
  kable_styling("striped", full_width = F)

```


## Machine Learning Application

One of the strengths of the PRA's supervisory infrastructure is that we are able to categorise firms based on their size, business model, area of operation etc. Each of these characteristics require their own supervisory approaches which the PRA can adapt to and enables more effective regulation. For my machine learning application I will look to cluster the banks into various distinct categories using principal component analysis so that we can better define them and our approach to them. I have chosen this cleaning approach before clustering because as a dimension reduction process it can help to extract the most important elements of each of the features and reduce the dataset into a more comprehensible size. With the multicolinearity problems that are shown at the start, this approach will mitigate this issue and improve cluster quality.  With *tidymodels* we can easily create a pipeline from input to PCA output


```{r PCA Pipeline, warning = FALSE}

pca_recipe <- recipes::recipe(firm ~ ., data = filtered_df)  %>%
  # step_naomit(all_numeric()) %>%
  # we only want to sue complete columns
  recipes::step_filter_missing(
    all_numeric(),
    threshold = 0) %>%
  # normalise all columns
  recipes::step_normalize(all_numeric()) %>%
  # compute principal components
  recipes::step_pca(
    all_numeric(), 
    threshold = .95
    )

pca_prep = recipes::prep(x = pca_recipe, training = filtered_df)

# plot this and cluster
pca_output_df = recipes::bake(object = pca_prep, filtered_df)

 kable(head(pca_output_df)) %>%
  kable_styling("striped", full_width = F)

```

```{r PCA Explained Variance chart, echo = FALSE}

as.data.frame(summary(pca_prep$steps[[3]]$res)$importance) %>%
  knitr::kable() %>%
  kable_styling("striped", full_width = F)

```


This provides us with 14 principal components that explain over 95% of the total variance of the dataset. This suggests that there is little explanatory power in this method however I will still investigate merits.

```{r, PCA Variance Chart, echo = FALSE}

pc_importance_df = as.data.frame(summary(pca_prep$steps[[3]]$res)$importance) %>%
  dplyr::filter(row.names(.) %in% c('Cumulative Proportion', 'Proportion of Variance')) %>%
  dplyr::mutate(Variance = case_when(
    row.names(.) == 'Cumulative Proportion' ~ 'Cumulative',
    row.names(.) == 'Proportion of Variance' ~ 'Individual'
  )) %>%
  tidyr::pivot_longer(
    cols = -'Variance', 
    names_to = 'PC'
  ) %>%
  tidyr::pivot_wider(
    id_cols = 'PC', 
    names_from = 'Variance', 
    values_from = 'value'
  ) %>%
  dplyr::mutate(PC = factor(PC, levels = glue::glue('PC{1:length(PC)}')))
  
ggplot(pc_importance_df)  + 
  geom_line(aes(x=PC, y=Cumulative, group = 1), linewidth=0.5)+
  geom_point(aes(x=PC, y=Cumulative, color="Cumulative"), pch =16)+
  geom_col(aes(x=PC, y=Individual, fill="Individual"), colour="black") +
  geom_hline(yintercept = 0.95, linetype="dashed", color = ggthemes::wsj_pal()(3)[3]) + 
  geom_text(label = '0.95',mapping = aes(x = 'PC9', y = 0.87), vjust=-1, color = ggthemes::wsj_pal()(3)[3]) +
  labs(
    title = 'Explained Variance', 
    subtitle = 'By Principal Component', 
    x = 'Principal Components', 
    y = 'Variance') +
  scale_color_manual(name ='', values=ggthemes::wsj_pal()(1)) +
  scale_fill_manual(name ='', values=ggthemes::wsj_pal()(2)[2]) +
  theme(legend.position="right")

```

I am now going to plot PC1 against PC2 to see if there are obvious clusters before performing k-means clustering.

```{r PC1 vs P2 Chart, echo = FALSE}

ggplot2::ggplot(
  data = pca_output_df, 
  mapping = aes(
    x = PC01, 
    y = PC02,
    col = ggthemes::wsj_pal()(1))) +
  geom_point() + 
  geom_bin2d(bins = 5, alpha = 0.4) +
  theme(legend.position = "none") +
  labs(
    title = 'Principal Component 1 against 2', 
    x = 'Principal Component 1', 
    y = 'Principal Component 2') 

```

There are few discernible groups using just PCs 1 and 2 alongside *ggplot2*'s *geom_bin2d* function to highlight clusters. I will use k-means clustering, an unsupervised ML technique used to group these points together minimising within-cluster variance and it is an incredibly efficient and explainable algorithm. The latter point is important when sharing findings with non-technical colleagues.

```{r PCA 1&2 Loadings, echo = FALSE}

tidy(pca_prep, 3) %>%
  filter(component %in% paste0("PC", 1:2)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~component, nrow = 1) +
  labs(
    title = 'Principal Component Loadings',
    subtitle = 'For PCs 1 and 2',
    y = 'Metrics',
    x = 'Loading value') 

```

What this shows us is the loadings of each of the PCs, we can see that PC1 is largely influenced by multiple of the metrics and PC2, the rest. As we expect, in cases where there is high *pure_net_claims_ratio* there will also be a high *pure_gross_claims_ratio* which matches our earlier charts. These two PC loadings agree with what we saw in the correlation matrix where there are examples of strong positive multicolinearity but no strong negative multicolinearity or indeeed just colinearity.

```{r K-Means Clustering, echo = F}

pca_points = pca_output_df %>% 
  dplyr::select(-firm)

kclusts <- 
  tibble(k = 3:8)  %>%
  mutate(
    kclust = map(k, ~kmeans(pca_points, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, pca_points)
    )

# provide the cluster positions of each group
clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

# provide the group ownership of each firm
assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented))

# clustering and their evaluation metric
clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))

ggplot(assignments, 
  aes(
    x = PC01, 
    y = PC02)) +
  geom_point(
    aes(
      color = .cluster), 
    alpha = 0.8) + 
  geom_point(
    data = clusters, 
    size = 8, 
    shape = "x") + 
  facet_wrap(~ k) +
  labs(
    title = 'PCA Clusters', 
    subtitle = 'By different K values', 
    x = 'Principal Component 1', 
    y = 'Principal Component 2',
    color = 'Cluster Group')

```

What this can allow us to do is 

```{r Total Withinss, echo = FALSE}

ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point(color = ggthemes::wsj_pal()(1)) +
  labs(
    title = 'Total Within Sum of Squares by K Value', 
    y = 'Total WSS', 
    x = '# of Groups')

```

There isn't an obvious 'elbow' here so there is no obvious optimal K value for grouping these columns. In order for PCA to be successful in either inference or prediction the data will need a more effective clean to allow for the algorithms to be successful.

## Application of Cloud Technologies

Once we have the capability to utilise cloud technologies, this project will receive multiple benefits from integrating such tools. Utilising *Azure Data Factory*, a hybrid data integration platform, will allow for smarter data pipelines and a singular connection for databases throghout the Bank. By having a central environment where one can access all data sources and all data submissions flow through we reduce the issues of siloing as all parties are aware of all data sources (even if access isn't immediately available due to security reasons). Currently, the Bank's data environment is such that




